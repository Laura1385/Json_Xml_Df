{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current python's version is:  3.9.17\n",
      "The path of the current Python interpreter:  /Users/laura/opt/anaconda3/envs/Tesina39/bin/python\n"
     ]
    }
   ],
   "source": [
    "#Check python's version\n",
    "from platform import python_version\n",
    "print(\"The current python's version is: \",python_version())\n",
    "\n",
    "#Check the path of the current Python interpreter used to execute the script.\n",
    "import sys\n",
    "print(\"The path of the current Python interpreter: \",sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warnings ist's install in the project.\n"
     ]
    }
   ],
   "source": [
    "#Check for library's version \n",
    "import pkg_resources\n",
    "\n",
    "library_name = 'warnings'\n",
    "try:\n",
    "    library_version = pkg_resources.get_distribution(library_name).version\n",
    "    print(f\"The version of {library_name} in use is: {library_version}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(f\"{library_name} ist's install in the project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#eaeaea;font-family:Calibri;color:#fe0b50;font-size:200%;text-align:center;border-radius: 50px;padding: 10px\">From Json e XML to df, cleansing and from df to Database</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Libraries & Utilities](#1)\n",
    "\n",
    "- Point A - Fetch/retrive data + Point B - Store Data into a structure\n",
    "    - [JSON](#2-1)\n",
    "    - [XML](#2-2)\n",
    "- Point C - Transform data \n",
    "    - [Cleaning](#3-1)\n",
    "    - [Tokenizzation/Splitting](#3-2)\n",
    "    - [Normalizzation (nlp library)](#3-3)\n",
    "- Point D - Final Files and Mongo Db  \n",
    "    - [Create a new files](#4-1)\n",
    "    - [Store data in Mongo](#4-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Libraries & Utilities</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/laura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Install all the libraries the project needs\n",
    "#pip install -r requirements.txt \n",
    "\n",
    "import os\n",
    "import json #Point A_JSON\n",
    "import xml.etree.ElementTree as ET #Point A_XML\n",
    "import pandas as pd #Point B\n",
    "import nltk #Point C Normalizzation\n",
    "nltk.download('punkt') #Point C Normalizzation\n",
    "from nltk.tokenize import word_tokenize #Point C Normalizzation\n",
    "import pymongo #Point D2_Mongo\n",
    "\n",
    "\n",
    "import warnings #only for os\n",
    "warnings.filterwarnings(\"ignore\") #only for os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:\n",
    "1. ***os***\n",
    "-> Used to interact with the operating system.\n",
    "2. ***json***\n",
    "-> Used to parse and read JSON type files and manipulate their tree structure.\n",
    "3. ***xml.etree.ElementTree***\n",
    "-> Used to parse and read JSON type files and manipulate their tree structure.\n",
    "4. ***pandas as pd***\n",
    "-> Used to parse data; among its main functionalities is the creation of the data structure called DataFrame.\n",
    "5. ***nltk***\n",
    "-> Used for natural language processing tasks such as tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "6. ***nltk.tokenize***\n",
    "-> Part of NLTK, it provides various tokenization methods for breaking text into individual words or sentences.\n",
    "7. ***pymongo***\n",
    "-> Python driver for MongoDB. It allows Python applications to interact with MongoDB databases, enabling operations such as inserting, updating, querying, and deleting data.\n",
    "8. ***warnings***\n",
    "-> Used to handle warnings during code execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point A - Fetch/retrive data + Point B - Store Data into a structure - JSON </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Calling the jsonfile function and printing the result\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df_j \u001b[38;5;241m=\u001b[39m \u001b[43mjsonfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(df_j.head(3))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#Delete empty coloumns \u001b[39;00m\n\u001b[1;32m     24\u001b[0m df_j\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mjsonfile\u001b[0;34m(name_json)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjsonfile\u001b[39m(name_json):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#Read and transform into a dictionary\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(name_json) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 10\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#Point B \u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#Store Data into a structure \u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#pd.json_normalize convert the JSON into a DataFrame\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "#Point A\n",
    "#Fetch/retrive data JSON\n",
    "\n",
    "name_json='Input/jsondata.json'\n",
    "\n",
    "#DEF that reads a JSON file and transforms it into a df by tagging columns\n",
    "def jsonfile(name_json):\n",
    "    #Read and transform into a dictionary\n",
    "    with open(name_json) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    #Point B \n",
    "    #Store Data into a structure \n",
    "    #pd.json_normalize convert the JSON into a DataFrame\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Calling the jsonfile function and printing the result\n",
    "df_j = jsonfile(name_json)\n",
    "#print(df_j.head(3))\n",
    "\n",
    "#Delete empty coloumns \n",
    "df_j.drop(columns=['json'], inplace=True)\n",
    "print(df_j.head(3))\n",
    "\n",
    "#Export to Excel\n",
    "df_j.to_excel('Dirty_JSON.xlsx', index=False) \n",
    "print(\"\\n\\nFile: Dirty_JSON.xlsx exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point A - Fetch/retrive data + Point B - Store Data into a structure - XML </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1   1       Nert  MacAskie  nmacaskie0@trellian.com  Agender  183.240.159.252   \n",
      "17  2      Moise   Jobbins     mjobbins1@jalbum.net     Male    132.3.198.177   \n",
      "27  3      Suzie  Lawlings      slawlings2@digg.com   Female     66.42.231.49   \n",
      "\n",
      "   only_digit only_char  \n",
      "1      jb5D80    2kpKME  \n",
      "17     z30NIA    bp9HJF  \n",
      "27     wrwPNM    1r0SWC  \n",
      "\n",
      "\n",
      "File: Dirty_XML.xlsx exported!\n"
     ]
    }
   ],
   "source": [
    "#Point A\n",
    "#Fetch/retrive data XML\n",
    "\n",
    "name_xml='Input/dataset.xml'\n",
    "\n",
    "#DEF that reads a XML file and transforms it into a df by tagging columns\n",
    "def xml(name_xml):\n",
    "    tree = ET.parse(name_xml)\n",
    "    #legge il file xml e restituisce l'oggetto di tipo ElementTree \n",
    "    #cioè la struttura gerarchica dell'albero XML\n",
    "  \n",
    "    root = tree.getroot() #oggetto \n",
    "    #ottiene il nodo radice dell'albero dell'oggetto\n",
    "    #cioè il punto di partenza per navigare dentro la struttura XML\n",
    "\n",
    "    tag=[] #Empty array/list\n",
    "    #popola l'array/lista\n",
    "    #Delete duplicate values \n",
    "    #(SET is dangerous because even if it cleans up duplicates, it messes up the order of the elements!!!)\n",
    "    for elem in root.iter():\n",
    "        if elem.tag not in tag: \n",
    "            tag.append(elem.tag)\n",
    "\n",
    "    #crea l'intestazione delle colonne con tag e un df vuoto\n",
    "    heading = list(tag)\n",
    "    df = pd.DataFrame(columns=heading)\n",
    "\n",
    "    rows_data = [] #crea array/lista vuota\n",
    "\n",
    "    #Itera sugli elementi XML e popola il DataFrame\n",
    "    for elem in root.iter():\n",
    "        data = {}\n",
    "        for tag in heading:\n",
    "            if elem.find(tag) is not None:\n",
    "                data[tag] = elem.find(tag).text\n",
    "            else:\n",
    "                data[tag] = None\n",
    "        # Aggiungi il dizionario dei dati della riga alla lista delle righe\n",
    "        rows_data.append(data)\n",
    "\n",
    "    #Point B \n",
    "    #Store Data into a structure \n",
    "    #Crea un nuovo DataFrame a partire dalle righe\n",
    "    df = pd.DataFrame(rows_data)\n",
    "\n",
    "    #Delete empty rows from id coloumn\n",
    "    df.dropna(subset=['id'], inplace=True)\n",
    "\n",
    "    #Delete empty coloumns \n",
    "    df.drop(columns=['dataset', 'json','record'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Calling the xml function and printing the result\n",
    "df_x = xml(name_xml)\n",
    "#sorted_dfxml = df_x.sort_values(by=['last_name'], ascending=False) #Sorted by last_name\n",
    "print(df_x.head(3))\n",
    "\n",
    "#Export to Excel\n",
    "df_x.to_excel('Dirty_XML.xlsx', index=False) \n",
    "print(\"\\n\\nFile: Dirty_XML.xlsx exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - CLEANING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The coloumns of df from Input JSON are:\n",
      " id             int64\n",
      "first_name    object\n",
      "last_name     object\n",
      "email         object\n",
      "gender        object\n",
      "ip_address    object\n",
      "only_digit    object\n",
      "only_char     object\n",
      "dtype: object \n",
      "\n",
      "\n",
      "Df JSON after creansing:\n",
      "    id first_name last_name             email gender     ip_address only_digit  \\\n",
      "0   1      riley    peddie  rpeddie0_ftc.gov   male  25 165 93 196         64   \n",
      "\n",
      "  only_char  \n",
      "0     dgyls  \n",
      "\n",
      "The coloumns of df from Input XML are:\n",
      " id            object\n",
      "first_name    object\n",
      "last_name     object\n",
      "email         object\n",
      "gender        object\n",
      "ip_address    object\n",
      "only_digit    object\n",
      "only_char     object\n",
      "dtype: object \n",
      "\n",
      "\n",
      "Df XML after creansing:\n",
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1  1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "\n",
      "  only_digit only_char  \n",
      "1        580     kpkme  \n"
     ]
    }
   ],
   "source": [
    "#Point C\n",
    "#Transform data - CLEANING (!@#$luca1984.!@# => luca)\n",
    "\n",
    "#JSON\n",
    "#DEF that does several cleaning \n",
    "dfc = df_j.copy()\n",
    "ty = 'JSON'\n",
    "def clng(dfc, ty):\n",
    "    #Check column type\n",
    "    print(f'\\nThe coloumns of df from Input {ty} are:\\n',dfc.dtypes,'\\n')\n",
    "\n",
    "    #Converts all df's values to lower case, if the type's columns is object\n",
    "    df_low = dfc.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)\n",
    "    \n",
    "    #Change '@' & '.' in '_'\n",
    "    df_low['email'] = df_low['email'].str.replace(r'[@]', '_', regex=True)\n",
    "    df_low['ip_address'] = df_low['ip_address'].str.replace(r'[.]', ' ', regex=True)\n",
    "\n",
    "    #Delete chars from 'only_digit' \n",
    "    df_low['only_digit'] = df_low['only_digit'].str.replace('[a-zA-Z]', '', regex=True)\n",
    "\n",
    "    #Delete numbers from 'only_char' \n",
    "    df_low['only_char'] = df_low['only_char'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "    print(f\"\\nDf {ty} after creansing:\\n\", (df_low.head(1)))\n",
    "\n",
    "    return df_low\n",
    "\n",
    "#Calling the DEF clng on JSON\n",
    "df_jcln = clng(dfc,ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF clng on XML \n",
    "dfc = df_x.copy()\n",
    "ty ='XML'\n",
    "df_xcln = clng(dfc,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - TOKENIZATION/SPLITTING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No duplicates JSON ;) \n",
      "\n",
      "Df JSON after Tokenization/Splitting:\n",
      "    id first_name last_name             email gender     ip_address only_digit  \\\n",
      "0   1      riley    peddie  rpeddie0_ftc.gov   male  25 165 93 196         64   \n",
      "\n",
      "  only_char email_name email_service email_extens  \n",
      "0     dgyls   rpeddie0           ftc         .gov  \n",
      "\n",
      "\n",
      "No duplicates XML ;) \n",
      "\n",
      "Df XML after Tokenization/Splitting:\n",
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1  1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "\n",
      "  only_digit only_char  email_name email_service email_extens  \n",
      "1        580     kpkme  nmacaskie0      trellian         .com  \n"
     ]
    }
   ],
   "source": [
    "#Point C\n",
    "#Transform data - TOKENIZATION/SPLITTING (xxx|yyy_zzz => xxx yyy zzz)\n",
    "\n",
    "#JSON\n",
    "#DEF that splitting email values \n",
    "dfc = df_jcln.copy()\n",
    "ty = 'JSON'\n",
    "def splt(dfc, ty):\n",
    "    #Creation of 3 new coloumns with 'mail' values \n",
    "    df_low = dfc.assign(email_name = dfc.email)\n",
    "    df_low0 = df_low.assign(email_service = df_low.email)\n",
    "    df_low1 = df_low0.assign(email_extens = df_low0.email)\n",
    "\n",
    "    #Splitting of email value \n",
    "    df_low1['email_name'] = df_low1['email_name'].str.split('[-_\\.]',expand = True,regex = True)[0]\n",
    "    df_low1['email_service'] = df_low1['email_service'].str.split('[-_\\.]',expand = True,regex = True)[1]\n",
    "    df_low1['email_extens'] = df_low1['email_extens'].str.extract(r'[^_.]*(\\..*)')\n",
    "\n",
    "    #Check duplicates\n",
    "    dup_df = df_low1[df_low1.duplicated()]\n",
    "\n",
    "    if dup_df.empty:\n",
    "        print(f'\\n\\nNo duplicates {ty} ;) \\n')\n",
    "        print(f\"Df {ty} after Tokenization/Splitting:\\n\", (df_low1.head(1)))\n",
    "    else:\n",
    "        print(dup_df)\n",
    "        \n",
    "    return df_low1\n",
    "\n",
    "#Calling the DEF splt on JSON\n",
    "df_jsplt = splt(dfc, ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF splt XML\n",
    "dfc = df_xcln.copy()\n",
    "ty = 'XML'\n",
    "df_xsplt = splt(dfc, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-3'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - NORMALIZATION (nlp library)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Df JSON after Normalization:\n",
      "    id first_name last_name                 email  gender      ip_address  \\\n",
      "0   1      riley    peddie      rpeddie0_ftc.gov    male   25 165 93 196   \n",
      "1   2   gabriela    godlee     ggodlee1_digg.com  female  156 228 163 91   \n",
      "2   3      major   fitchen  mfitchen2_rambler.ru    male    203 95 8 234   \n",
      "\n",
      "  only_digit only_char email_name email_service email_extens  \\\n",
      "0         64     dgyls   rpeddie0           ftc         .gov   \n",
      "1        681       yoz   ggodlee1          digg         .com   \n",
      "2         88      qlxj  mfitchen2       rambler          .ru   \n",
      "\n",
      "                 email_tokens  \n",
      "0      [rpeddie0, @, ftc.gov]  \n",
      "1     [ggodlee1, @, digg.com]  \n",
      "2  [mfitchen2, @, rambler.ru]  \n",
      "\n",
      "Df XML after Normalization:\n",
      "    id first_name last_name                    email   gender       ip_address  \\\n",
      "1   1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "17  2      moise   jobbins     mjobbins1_jalbum.net     male    132 3 198 177   \n",
      "27  3      suzie  lawlings      slawlings2_digg.com   female     66 42 231 49   \n",
      "\n",
      "   only_digit only_char  email_name email_service email_extens  \\\n",
      "1         580     kpkme  nmacaskie0      trellian         .com   \n",
      "17         30     bphjf   mjobbins1        jalbum         .net   \n",
      "27                 rswc  slawlings2          digg         .com   \n",
      "\n",
      "                     email_tokens  \n",
      "1   [nmacaskie0, @, trellian.com]  \n",
      "17     [mjobbins1, @, jalbum.net]  \n",
      "27      [slawlings2, @, digg.com]  \n"
     ]
    }
   ],
   "source": [
    "#Che fare? Estrarre dati da colonna email? nome e estensione?\n",
    "\n",
    "#Point C\n",
    "#Transform data - NORMALIZATION (nlp library)\n",
    "#pip install nltk\n",
    "\n",
    "#Apply() applica la tokenizzazione alle celle della colonna 'email'. \n",
    "#Lambda prende i valori della colonna 'email' (x) e applica la funzione word_tokenize() per suddividere in token (parole). \n",
    "#I token vengono salvati nella nuova colonna 'email_tokens' nel df.\n",
    "\n",
    "#JSON\n",
    "#DEF that applied the normalizzation to email value\n",
    "dfc = df_jsplt.copy()\n",
    "ty = 'JSON'\n",
    "old_df = df_j\n",
    "def norm(dfc, ty, old_df):\n",
    "    dfc['email_tokens'] = old_df['email'].apply(lambda x: word_tokenize(x)) #Take the first df created for have the entire mail adress\n",
    "    print(f\"\\nDf {ty} after Normalization:\\n\", (dfc.head(3)))\n",
    "\n",
    "    return dfc\n",
    "\n",
    "#Calling the DEF norm on JSON\n",
    "df_jnorm = norm(dfc, ty, old_df)\n",
    "\n",
    "#XML\n",
    "#Using the DEF norm on XML\n",
    "dfc = df_xsplt.copy()\n",
    "ty = 'XML'\n",
    "old_df = df_x\n",
    "\n",
    "df_xnorm = norm(dfc, ty, old_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Create a new File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Final_df_JSON.xlsx' and 'Final_df_XML.xlsx' exported!\n",
      "\n",
      "\n",
      "Done! File Dirty_JSON.xlsx and e Dirty_XML deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Point D1\n",
    "#Create a New file\n",
    "\n",
    "df_jnorm.to_excel('Final_df_JSON.xlsx', index=False) \n",
    "df_xnorm.to_excel('Final_df_XML.xlsx', index=False) \n",
    "\n",
    "print(\"\\n'Final_df_JSON.xlsx' and 'Final_df_XML.xlsx' exported!\")\n",
    "\n",
    "#Chose if you want delete 'Dirty' files\n",
    "response = input(\"\\nDo you want to delete Dirty_JSON.xlsx e Dirty_XML.xlsx ? (y/n): \")\n",
    "response_lower = response.lower() #Conver in lower case\n",
    "\n",
    "if response_lower == 'y':\n",
    "    try:\n",
    "        os.remove('Dirty_JSON.xlsx')\n",
    "        os.remove('Dirty_XML.xlsx')\n",
    "        print(\"\\n\\nDone! File Dirty_JSON.xlsx and e Dirty_XML deleted\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nHey! There isn't any 'dirty' files!'-_- \\n\")\n",
    "else:  \n",
    "        print(\"\\n\\nOk, 'dirty' files not deleted.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Store data in Mongo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe JSON exported to MongoDB!\n",
      "\n",
      "Dataframe XML exported to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "#Point D2\n",
    "#Store data into DB MONGO\n",
    "#https://stackoverflow.com/questions/20167194/insert-a-pandas-dataframe-into-mongodb-using-pymongo\n",
    "\n",
    "#pip install pymongo\n",
    "\n",
    "#DEF that store JSON df in Mongo DB\n",
    "db_name = \"Database_Json\"\n",
    "db_collection = \"df_jexp\"\n",
    "dfc = df_jnorm.copy()\n",
    "ty = 'JSON'\n",
    "\n",
    "def mongo (db_name, db_collection, dfc, ty):\n",
    "    myclient1 = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mydb1 = myclient1[db_name]\n",
    "    mycol1 = mydb1[db_collection]\n",
    "\n",
    "    df = dfc\n",
    "    fields = df.to_dict('records')\n",
    "    mycol1.insert_many(fields) \n",
    "\n",
    "    print(f\"\\nDataframe {ty} exported to MongoDB!\")\n",
    "\n",
    "#Calling the DEF mongo on JSON\n",
    "mongo_j = mongo (db_name, db_collection, dfc, ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF mongo on XML\n",
    "db_name = \"Database_XML\"\n",
    "db_collection = \"df_xexp\"\n",
    "dfc = df_xnorm.copy()\n",
    "ty = 'XML'\n",
    "\n",
    "mongo_x = mongo (db_name, db_collection, dfc, ty)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
