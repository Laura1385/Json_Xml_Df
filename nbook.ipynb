{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current python's version is:  3.9.17\n",
      "The path of the current Python interpreter:  /Users/laura/opt/anaconda3/envs/Tesina39/bin/python\n"
     ]
    }
   ],
   "source": [
    "#Check python's version\n",
    "from platform import python_version\n",
    "print(\"The current python's version is: \",python_version())\n",
    "\n",
    "#Check the path of the current Python interpreter used to execute the script.\n",
    "import sys\n",
    "print(\"The path of the current Python interpreter: \",sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warnings ist's install in the project.\n"
     ]
    }
   ],
   "source": [
    "#Check for library's version \n",
    "import pkg_resources\n",
    "\n",
    "library_name = 'warnings'\n",
    "try:\n",
    "    library_version = pkg_resources.get_distribution(library_name).version\n",
    "    print(f\"The version of {library_name} in use is: {library_version}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(f\"{library_name} ist's install in the project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#eaeaea;font-family:Calibri;color:#fe0b50;font-size:200%;text-align:center;border-radius: 50px;padding: 10px\">From Json e XML to df, cleansing and from df to Database</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Libraries & Utilities](#1)\n",
    "\n",
    "- Point A - Retrive data + Point B - Store Data into a structure\n",
    "    - [JSON](#2-1)\n",
    "    - [XML](#2-2)\n",
    "- Point C - Transform data \n",
    "    - [Cleaning](#3-1)\n",
    "    - [Tokenizzation/Splitting](#3-2)\n",
    "    - [Normalizzation (nlp library)](#3-3)\n",
    "- Point D - Final Files and Mongo Db  \n",
    "    - [Create a new files](#4-1)\n",
    "    - [Store data in Mongo](#4-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Libraries & Utilities</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/laura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Install all the libraries the project needs\n",
    "#pip install -r requirements.txt \n",
    "\n",
    "import os\n",
    "import json #Point A_JSON\n",
    "import xml.etree.ElementTree as ET #Point A_XML\n",
    "import pandas as pd #Point B\n",
    "import nltk #Point C Normalizzation\n",
    "nltk.download('punkt') #Point C Normalizzation\n",
    "from nltk.tokenize import word_tokenize #Point C Normalizzation\n",
    "import pymongo #Point D2_Mongo\n",
    "\n",
    "import warnings #only for os\n",
    "warnings.filterwarnings(\"ignore\") #only for os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:\n",
    "1. ***os***\n",
    "-> Used to interact with the operating system.\n",
    "2. ***json***\n",
    "-> Used to parse and read JSON type files and manipulate their tree structure.\n",
    "3. ***xml.etree.ElementTree***\n",
    "-> Used to parse and read JSON type files and manipulate their tree structure.\n",
    "4. ***pandas as pd***\n",
    "-> Used to parse data; among its main functionalities is the creation of the data structure called DataFrame.\n",
    "5. ***nltk***\n",
    "-> Used for natural language processing tasks such as tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "6. ***nltk.tokenize***\n",
    "-> Part of NLTK, it provides various tokenization methods for breaking text into individual words or sentences.\n",
    "7. ***pymongo***\n",
    "-> Python driver for MongoDB. It allows Python applications to interact with MongoDB databases, enabling operations such as inserting, updating, querying, and deleting data.\n",
    "8. ***warnings***\n",
    "-> Used to handle warnings during code execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point A - Retrive data + Point B - Store Data into a structure - JSON </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id first_name last_name                 email  gender      ip_address  \\\n",
      "0   1      Riley    Peddie      rpeddie0@ftc.gov    Male   25.165.93.196   \n",
      "1   2   Gabriela    Godlee     ggodlee1@digg.com  Female  156.228.163.91   \n",
      "2   3      Major   Fitchen  mfitchen2@rambler.ru    Male    203.95.8.234   \n",
      "\n",
      "  only_digit only_char  \n",
      "0     bnn6P4    dg1YLS  \n",
      "1     k6h8Z1    yo204Z  \n",
      "2     s8kSG8    q4lX1J  \n",
      "\n",
      "\n",
      "File: Dirty_JSON.xlsx exported!\n"
     ]
    }
   ],
   "source": [
    "#Point A\n",
    "#Retrives data JSON\n",
    "\n",
    "name_json='Input/jsondata.json'\n",
    "\n",
    "#DEF that reads a JSON file and transforms it into a df by tagging columns\n",
    "def jsonfile(name_json):\n",
    "    #Reads and transforms into a dictionary\n",
    "    with open(name_json) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    #Point B \n",
    "    #Stores Data into a structure \n",
    "    #pd.json_normalize converts the JSON into a DataFrame\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Calls the jsonfile function and prints the result\n",
    "df_j = jsonfile(name_json)\n",
    "#print(df_j.head(3))\n",
    "\n",
    "#Deletes empty coloumns \n",
    "df_j.drop(columns=['json'], inplace=True)\n",
    "print(df_j.head(3))\n",
    "\n",
    "#Export to Excel\n",
    "df_j.to_excel('Dirty_JSON.xlsx', index=False) \n",
    "print(\"\\n\\nFile: Dirty_JSON.xlsx exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point A - Retrive data + Point B - Store Data into a structure - XML </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1   1       Nert  MacAskie  nmacaskie0@trellian.com  Agender  183.240.159.252   \n",
      "17  2      Moise   Jobbins     mjobbins1@jalbum.net     Male    132.3.198.177   \n",
      "27  3      Suzie  Lawlings      slawlings2@digg.com   Female     66.42.231.49   \n",
      "\n",
      "   only_digit only_char  \n",
      "1      jb5D80    2kpKME  \n",
      "17     z30NIA    bp9HJF  \n",
      "27     wrwPNM    1r0SWC  \n",
      "\n",
      "\n",
      "File: Dirty_XML.xlsx exported!\n"
     ]
    }
   ],
   "source": [
    "#Point A\n",
    "#Retrives data XML\n",
    "\n",
    "name_xml='Input/dataset.xml'\n",
    "\n",
    "#DEF that reads a XML file and transforms it into a df by tagging columns\n",
    "def xml(name_xml):\n",
    "    tree = ET.parse(name_xml)\n",
    "    #Reads xml file and returns the object of typ\n",
    "    #(the hierarchical structure of the XML tree)\n",
    "  \n",
    "    root = tree.getroot()\n",
    "    #Obtains the root node of the object tree\n",
    "    #(the starting point for navigating within the XML structure)\n",
    "\n",
    "    tag=[] #Empty array/list\n",
    "    #Populates the array/list and deletes duplicate values \n",
    "    #(SET is dangerous because even if it cleans up duplicates, it messes up the order of the elements!!!)\n",
    "    for elem in root.iter():\n",
    "        if elem.tag not in tag: \n",
    "            tag.append(elem.tag)\n",
    "\n",
    "    #Creates column header with tags and an empty df\n",
    "    heading = list(tag)\n",
    "    df = pd.DataFrame(columns=heading)\n",
    "\n",
    "    rows_data = []\n",
    "    #Iterates on XML elements and populates the DataFrame\n",
    "    for elem in root.iter():\n",
    "        data = {}\n",
    "        for tag in heading:\n",
    "            if elem.find(tag) is not None:\n",
    "                data[tag] = elem.find(tag).text\n",
    "            else:\n",
    "                data[tag] = None\n",
    "        #Adds the row data dictionary to the list of rows\n",
    "        rows_data.append(data)\n",
    "\n",
    "    #Point B \n",
    "    #Stores Data into a structure \n",
    "    #Creates a new DataFrame from the rows\n",
    "    df = pd.DataFrame(rows_data)\n",
    "\n",
    "    #Deletes empty rows from id coloumn\n",
    "    df.dropna(subset=['id'], inplace=True)\n",
    "\n",
    "    #Deletes empty coloumns \n",
    "    df.drop(columns=['dataset', 'json','record'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Calls the xml function and prints the result\n",
    "df_x = xml(name_xml)\n",
    "#sorted_dfxml = df_x.sort_values(by=['last_name'], ascending=False) #Sorted by last_name\n",
    "print(df_x.head(3))\n",
    "\n",
    "#Export to Excel\n",
    "df_x.to_excel('Dirty_XML.xlsx', index=False) \n",
    "print(\"\\n\\nFile: Dirty_XML.xlsx exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - CLEANING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The coloumns of df from Input JSON are:\n",
      " id             int64\n",
      "first_name    object\n",
      "last_name     object\n",
      "email         object\n",
      "gender        object\n",
      "ip_address    object\n",
      "only_digit    object\n",
      "only_char     object\n",
      "dtype: object \n",
      "\n",
      "\n",
      "Df JSON after creansing:\n",
      "    id first_name last_name             email gender     ip_address only_digit  \\\n",
      "0   1      riley    peddie  rpeddie0_ftc.gov   male  25 165 93 196         64   \n",
      "\n",
      "  only_char  \n",
      "0     dgyls  \n",
      "\n",
      "The coloumns of df from Input XML are:\n",
      " id            object\n",
      "first_name    object\n",
      "last_name     object\n",
      "email         object\n",
      "gender        object\n",
      "ip_address    object\n",
      "only_digit    object\n",
      "only_char     object\n",
      "dtype: object \n",
      "\n",
      "\n",
      "Df XML after creansing:\n",
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1  1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "\n",
      "  only_digit only_char  \n",
      "1        580     kpkme  \n"
     ]
    }
   ],
   "source": [
    "#Point C\n",
    "#Transforms data - CLEANING (!@#$luca1984.!@# => luca)\n",
    "\n",
    "#JSON\n",
    "#DEF that does several cleaning \n",
    "dfc = df_j.copy()\n",
    "ty = 'JSON'\n",
    "def clng(dfc, ty):\n",
    "    #Checks column type\n",
    "    print(f'\\nThe coloumns of df from Input {ty} are:\\n',dfc.dtypes,'\\n')\n",
    "\n",
    "    #Converts all df's values to lower case, if the type's columns is object\n",
    "    df_low = dfc.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)\n",
    "    \n",
    "    #Change '@' & '.' in '_'\n",
    "    df_low['email'] = df_low['email'].str.replace(r'[@]', '_', regex=True)\n",
    "    df_low['ip_address'] = df_low['ip_address'].str.replace(r'[.]', ' ', regex=True)\n",
    "\n",
    "    #Deletes chars from 'only_digit' \n",
    "    df_low['only_digit'] = df_low['only_digit'].str.replace('[a-zA-Z]', '', regex=True)\n",
    "\n",
    "    #Deletes numbers from 'only_char' \n",
    "    df_low['only_char'] = df_low['only_char'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "    print(f\"\\nDf {ty} after creansing:\\n\", (df_low.head(1)))\n",
    "\n",
    "    return df_low\n",
    "\n",
    "#Calls the DEF clng on JSON\n",
    "df_jcln = clng(dfc,ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF clng on XML \n",
    "dfc = df_x.copy()\n",
    "ty ='XML'\n",
    "df_xcln = clng(dfc,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - TOKENIZATION/SPLITTING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No duplicates JSON ;) \n",
      "\n",
      "Df JSON after Tokenization/Splitting:\n",
      "    id first_name last_name             email gender     ip_address only_digit  \\\n",
      "0   1      riley    peddie  rpeddie0_ftc.gov   male  25 165 93 196         64   \n",
      "\n",
      "  only_char email_name email_service email_extens  \n",
      "0     dgyls   rpeddie0           ftc         .gov  \n",
      "\n",
      "\n",
      "No duplicates XML ;) \n",
      "\n",
      "Df XML after Tokenization/Splitting:\n",
      "   id first_name last_name                    email   gender       ip_address  \\\n",
      "1  1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "\n",
      "  only_digit only_char  email_name email_service email_extens  \n",
      "1        580     kpkme  nmacaskie0      trellian         .com  \n"
     ]
    }
   ],
   "source": [
    "#Point C\n",
    "#Transforms data - TOKENIZATION/SPLITTING (xxx|yyy_zzz => xxx yyy zzz)\n",
    "\n",
    "#JSON\n",
    "#DEF that splitting email's values \n",
    "dfc = df_jcln.copy()\n",
    "ty = 'JSON'\n",
    "def splt(dfc, ty):\n",
    "    #Creations of 3 new coloumns with 'mail' values \n",
    "    df_low = dfc.assign(email_name = dfc.email)\n",
    "    df_low0 = df_low.assign(email_service = df_low.email)\n",
    "    df_low1 = df_low0.assign(email_extens = df_low0.email)\n",
    "\n",
    "    #Splitting email's value \n",
    "    df_low1['email_name'] = df_low1['email_name'].str.split('[-_\\.]',expand = True,regex = True)[0]\n",
    "    df_low1['email_service'] = df_low1['email_service'].str.split('[-_\\.]',expand = True,regex = True)[1]\n",
    "    df_low1['email_extens'] = df_low1['email_extens'].str.extract(r'[^_.]*(\\..*)')\n",
    "\n",
    "    #Checks duplicates\n",
    "    dup_df = df_low1[df_low1.duplicated()]\n",
    "\n",
    "    if dup_df.empty:\n",
    "        print(f'\\n\\nNo duplicates {ty} ;) \\n')\n",
    "        print(f\"Df {ty} after Tokenization/Splitting:\\n\", (df_low1.head(1)))\n",
    "    else:\n",
    "        print(dup_df)\n",
    "        \n",
    "    return df_low1\n",
    "\n",
    "#Calls the DEF splt on JSON\n",
    "df_jsplt = splt(dfc, ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF splt XML\n",
    "dfc = df_xcln.copy()\n",
    "ty = 'XML'\n",
    "df_xsplt = splt(dfc, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3-3'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point C - Transform data - NORMALIZATION (nlp library)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Df JSON after Normalization:\n",
      "    id first_name last_name                 email  gender      ip_address  \\\n",
      "0   1      riley    peddie      rpeddie0_ftc.gov    male   25 165 93 196   \n",
      "1   2   gabriela    godlee     ggodlee1_digg.com  female  156 228 163 91   \n",
      "2   3      major   fitchen  mfitchen2_rambler.ru    male    203 95 8 234   \n",
      "\n",
      "  only_digit only_char email_name email_service email_extens  \\\n",
      "0         64     dgyls   rpeddie0           ftc         .gov   \n",
      "1        681       yoz   ggodlee1          digg         .com   \n",
      "2         88      qlxj  mfitchen2       rambler          .ru   \n",
      "\n",
      "                 email_tokens  \n",
      "0      [rpeddie0, @, ftc.gov]  \n",
      "1     [ggodlee1, @, digg.com]  \n",
      "2  [mfitchen2, @, rambler.ru]  \n",
      "\n",
      "Df XML after Normalization:\n",
      "    id first_name last_name                    email   gender       ip_address  \\\n",
      "1   1       nert  macaskie  nmacaskie0_trellian.com  agender  183 240 159 252   \n",
      "17  2      moise   jobbins     mjobbins1_jalbum.net     male    132 3 198 177   \n",
      "27  3      suzie  lawlings      slawlings2_digg.com   female     66 42 231 49   \n",
      "\n",
      "   only_digit only_char  email_name email_service email_extens  \\\n",
      "1         580     kpkme  nmacaskie0      trellian         .com   \n",
      "17         30     bphjf   mjobbins1        jalbum         .net   \n",
      "27                 rswc  slawlings2          digg         .com   \n",
      "\n",
      "                     email_tokens  \n",
      "1   [nmacaskie0, @, trellian.com]  \n",
      "17     [mjobbins1, @, jalbum.net]  \n",
      "27      [slawlings2, @, digg.com]  \n"
     ]
    }
   ],
   "source": [
    "#Che fare? Estrarre dati da colonna email? nome e estensione?\n",
    "\n",
    "#Point C\n",
    "#Transforms data - NORMALIZATION (nlp library)\n",
    "#pip install nltk\n",
    "\n",
    "#Apply() applies tokenization to the cells in the 'email' column. \n",
    "#Lambda takes the values from the 'email' column (x) and applies the word_tokenize() function to tokenize the tokens (words). \n",
    "#The tokens are saved in the new column 'email_tokens' in the df.\n",
    "\n",
    "#JSON\n",
    "#DEF that applies the normalizzation to email's\n",
    "dfc = df_jsplt.copy()\n",
    "ty = 'JSON'\n",
    "old_df = df_j\n",
    "def norm(dfc, ty, old_df):\n",
    "    dfc['email_tokens'] = old_df['email'].apply(lambda x: word_tokenize(x)) #Take the first df created for have the entire mail adress\n",
    "    print(f\"\\nDf {ty} after Normalization:\\n\", (dfc.head(3)))\n",
    "\n",
    "    return dfc\n",
    "\n",
    "#Calls the DEF norm on JSON\n",
    "df_jnorm = norm(dfc, ty, old_df)\n",
    "\n",
    "#XML\n",
    "#Using the DEF norm on XML\n",
    "dfc = df_xsplt.copy()\n",
    "ty = 'XML'\n",
    "old_df = df_x\n",
    "\n",
    "df_xnorm = norm(dfc, ty, old_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4-1'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point D - Create a new File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Final_df_JSON.xlsx' and 'Final_df_XML.xlsx' exported!\n",
      "\n",
      "\n",
      "Done! File Dirty_JSON.xlsx and e Dirty_XML deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Point D1\n",
    "#Creates a New file\n",
    "\n",
    "df_jnorm.to_excel('Final_df_JSON.xlsx', index=False) \n",
    "df_xnorm.to_excel('Final_df_XML.xlsx', index=False) \n",
    "\n",
    "print(\"\\n'Final_df_JSON.xlsx' and 'Final_df_XML.xlsx' exported!\")\n",
    "\n",
    "#Chose if you want delete 'Dirty' files\n",
    "response = input(\"\\nDo you want to delete Dirty_JSON.xlsx e Dirty_XML.xlsx ? (y/n): \")\n",
    "response_lower = response.lower() #Conver in lower case\n",
    "\n",
    "if response_lower == 'y':\n",
    "    try:\n",
    "        os.remove('Dirty_JSON.xlsx')\n",
    "        os.remove('Dirty_XML.xlsx')\n",
    "        print(\"\\n\\nDone! File Dirty_JSON.xlsx and e Dirty_XML deleted\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nHey! There isn't any 'dirty' files!'-_- \\n\")\n",
    "else:  \n",
    "        print(\"\\n\\nOk, 'dirty' files not deleted.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4-2'></a>\n",
    "<h1 style=\"font-size: 150%;\">Point D - Store data in Mongo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe JSON exported to MongoDB!\n",
      "\n",
      "Dataframe XML exported to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "#Point D2\n",
    "#Store data into DB MONGO\n",
    "#https://stackoverflow.com/questions/20167194/insert-a-pandas-dataframe-into-mongodb-using-pymongo\n",
    "\n",
    "#pip install pymongo\n",
    "\n",
    "#DEF that stores JSON df in Mongo DB\n",
    "db_name = \"Database_Json\"\n",
    "db_collection = \"df_jexp\"\n",
    "dfc = df_jnorm.copy()\n",
    "ty = 'JSON'\n",
    "\n",
    "def mongo (db_name, db_collection, dfc, ty):\n",
    "    myclient1 = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mydb1 = myclient1[db_name]\n",
    "    mycol1 = mydb1[db_collection]\n",
    "\n",
    "    df = dfc\n",
    "    fields = df.to_dict('records')\n",
    "    mycol1.insert_many(fields) \n",
    "\n",
    "    print(f\"\\nDataframe {ty} exported to MongoDB!\")\n",
    "\n",
    "#Calls the DEF mongo on JSON\n",
    "mongo_j = mongo (db_name, db_collection, dfc, ty)\n",
    "\n",
    "#XML\n",
    "#Using the DEF mongo on XML\n",
    "db_name = \"Database_XML\"\n",
    "db_collection = \"df_xexp\"\n",
    "dfc = df_xnorm.copy()\n",
    "ty = 'XML'\n",
    "\n",
    "mongo_x = mongo (db_name, db_collection, dfc, ty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
